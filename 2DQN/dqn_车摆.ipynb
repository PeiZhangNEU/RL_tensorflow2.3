{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        n_features,\n",
    "        learning_rate=0.01,\n",
    "        reward_decay=0.9,\n",
    "        e_greedy = 0.9,\n",
    "        replace_target_iter=300,\n",
    "        memory_size=500,\n",
    "        batch_size=32,\n",
    "        e_greedy_increment=None, #这个是设置epsilon的变化与否的，它是nan证明epsilon一直不变\n",
    "        output_graph=False\n",
    "    ):\n",
    "        '''四个动作数量，两个状态数量'''\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        #如果e_greedy_increment不是none，那么刚开始动作是瞎选的，随着learn的次数增多\n",
    "        #epsilon值慢慢增加，每次增加e_greedy_increment直到最大，使得随机选动作的概率变小\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "        \n",
    "        # 总共的学习步数，执行一次learn函数，这个值才加一\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # 初始化[s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, n_features*2 + 2))  #500行,有6列：s占2列，a占1列，r占1列，s_占2列；\n",
    "        \n",
    "        # [target_net, evalunet]\n",
    "        self._build_net()   #因为self.target_net和self.evalunet这两个网络是在这个方法里产生的，别的方法要去调用他们，\n",
    "        #要么这两个网络在别的方法里先被运行出来，就可以被调用了\n",
    "        #要么这两个网络一开始就变成全局的变量，不用别的方法每次都运行一遍。！\n",
    "        \n",
    "        self.cost_his = []     #这一句是创造一个记录损失的列表\n",
    "        \n",
    "    def _build_net(self):\n",
    "        '''建立预测模型和target模型'''\n",
    "        #----------------------------------build evaluate model--------------------------------\n",
    "        # 输入状态，None,2 ;输出动作对应的值 None,4 四个动作每个都对应一个动作价值\n",
    "        s = tf.keras.layers.Input(shape=(self.n_features,), name='s')\n",
    "        q_target = tf.keras.layers.Input(shape=(self.n_actions,), name='q_target')\n",
    "        \n",
    "        # 预测模型\n",
    "        x = tf.keras.layers.Dense(20, activation='relu', name='l1')(s)\n",
    "        x = tf.keras.layers.Dense(self.n_actions, name='l2')(x)\n",
    "        self.eval_net = tf.keras.Model(inputs=s, outputs=x)\n",
    "        \n",
    "        #损失函数\n",
    "        self.loss = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        #优化器\n",
    "        self._train_op = tf.keras.optimizers.RMSprop(learning_rate=self.lr)\n",
    "        \n",
    "        #-----------------------------------build target model------------------------------------\n",
    "        #目标模型就是一个框架，他不需要训练，只需要把预测模型里面的权重往里更新即可\n",
    "        s_ = tf.keras.layers.Input(shape=(self.n_features,), name='s_')\n",
    "        \n",
    "        #目标模型\n",
    "        x = tf.keras.layers.Dense(20, activation='relu', name='l1')(s_)\n",
    "        x = tf.keras.layers.Dense(self.n_actions, name='l2')(x)\n",
    "        self.target_net = tf.keras.Model(inputs=s_, outputs=x)\n",
    "        \n",
    "    def replace_target(self):\n",
    "        '''预测模型的权重更新到目标模型'''\n",
    "        #使用set_weights和get_weights的方法对每层的权重进行复制转移\n",
    "        self.target_net.get_layer(name='l1').set_weights(self.eval_net.get_layer(name='l1').get_weights())\n",
    "        self.target_net.get_layer(name='l2').set_weights(self.eval_net.get_layer(name='l2').get_weights())\n",
    "        \n",
    "    def store_transition(self, s, a, r, s_):  #这里传入的a是一个动作！\n",
    "        '''存入记忆库'''\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            #返回对象是否具有具有给定名称的属性。从上面来看，是没有的，那么hasattr会返回FALSE，也就是说如果没有，就会执行下面语句\n",
    "            self.memory_counter = 0\n",
    "        \n",
    "        transition = np.hstack((s, [a,r], s_))\n",
    "        \n",
    "        # 替换旧的memory，当memory计数500次，下一次就会更新memory的第一行，。。。\n",
    "        index = self.memory_counter % self.memory_size  #取余\n",
    "        self.memory[index, :] = transition\n",
    "        \n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        '''选择动作的方法'''\n",
    "        observation = observation[np.newaxis, :]  \n",
    "        #在s前面增加一个维度，如s原来是[2,]现在变成[1,2]，因为要这样才能符合模型的要求，因为模型要求[None,2]和RNN北京天气讲的单个预测方法一样\n",
    "        \n",
    "        # 小于epsilon时候按照动作价值选,用模型预测出四个动作价值，转成numpy之后用最大索引\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            actions_value = self.eval_net(observation).numpy()\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        '''从记忆库学习'''\n",
    "        print('learn step is:',self.learn_step_counter)\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0: \n",
    "            #每replace_target次学习更换一次参数,也就是把target模型的参数更新成和evaluate模型参数\n",
    "            self.replace_target()\n",
    "            print('\\n目标模型的参数已经被替换。')\n",
    "            \n",
    "        # 从memory里面采样\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            #如果memory存储次数大于了memory表的长度（500）\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)  #从500里面选择32个整数作为索引\n",
    "        else:\n",
    "            #会不会出现memorycounter<32的情况，首先让memory记录多少，这是在主循环里面设定的\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size) #从memorycounter范围选取32个整数作为索引\n",
    "        \n",
    "        batch_memory = self.memory[sample_index, :]  #从memory表里面取出上面32个索引对应的行作为batchmemory\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # batch_memory一共32行，6列，前2列s,action1列，r1列，最后两列s_.所以是把s_放到target网络里面，得到q_next[32,4],一个q表\n",
    "            \n",
    "            #生成之前的结果，是以前的模型生成的，是一个[32,4]的tensor\n",
    "            q_next = self.target_net(batch_memory[:, -self.n_features:]).numpy() #模型输入：所有行，最后两列s_。q_target随便，因为它不用训练\n",
    "            \n",
    "            #生成预测结果，是现在的模型生成的, 是一个[32,4]的tensor\n",
    "            q_eval = self.eval_net(batch_memory[:, :self.n_features])      #这里不能直接.numpy()，因为会使他不是tensor，造成无法计算梯度\n",
    "            #模型输入：所有行，前两列s\n",
    "            \n",
    "            #改变q_target,q_target[32,4]\n",
    "            q_target = q_eval.numpy()\n",
    "            batch_index = np.arange(self.batch_size, dtype=np.int32)       #0到31\n",
    "            eval_act_index = batch_memory[:, self.n_features].astype(int)  #预测的动作:所有行，index=2也就是第三列.[32,]\n",
    "            reward = batch_memory[:, self.n_features+1]                    #奖励，index=3也就是第4列,[32,]\n",
    "            \n",
    "            #根据奖励更新当前的预测结果:这里怎么选的？ 每行按照动作index选一个，最后形状是[32,]，对负数的动作如-3也可以选，就是往左数，\n",
    "            #不够用了再蹦到右边往左数\n",
    "            q_target[batch_index, eval_act_index]  = reward + self.gamma * np.max(q_next, axis=1) #左右最大，也就是这一行的最大值\n",
    "            \n",
    "            #计算损失\n",
    "            self.cost = self.loss(y_true=q_target, y_pred=q_eval)\n",
    "            \n",
    "        #计算梯度\n",
    "        gradients = tape.gradient(self.cost, self.eval_net.trainable_variables)\n",
    "        #优化器梯度下降\n",
    "        self._train_op.apply_gradients(zip(gradients, self.eval_net.trainable_variables))\n",
    "        #记录损失\n",
    "        self.cost_his.append(self.cost)\n",
    "        \n",
    "        # increasing epsilon\n",
    "        # 随机概率随着训练次数减少，默认不变\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1  #只要执行一次learn函数，这个就会加一\n",
    "    \n",
    "    def plot_cost(self):\n",
    "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('steps')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL = DeepQNetwork(n_actions=env.action_space.n,\n",
    "                  n_features=env.observation_space.shape[0],\n",
    "                  learning_rate=0.01,\n",
    "                  e_greedy=0.9,\n",
    "                  replace_target_iter=100,\n",
    "                  memory_size=2000,\n",
    "                  e_greedy_increment=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totle_steps = 0\n",
    "\n",
    "for episode in range(1000):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        env.render()\n",
    "        action = RL.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action) #还有一个叫做info\n",
    "        \n",
    "        #修改一下默认env里面的奖励函数，怎么知道怎么改，pycharm里面搜索源文件\n",
    "        # the smaller theta and closer to center the better\n",
    "        x, x_dot, theta, theta_dot = observation_\n",
    "        r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2\n",
    "        \n",
    "        RL.store_transition(observation, action, reward, observation_)\n",
    "        \n",
    "        if (totle_steps>50) and (totle_steps%5==0):\n",
    "            RL.learn()\n",
    "        \n",
    "        if done:\n",
    "            print('eposode: ', episode)\n",
    "            print('epsilon: ',round(RL.epsilon,2))\n",
    "            break\n",
    "            \n",
    "        observation = observation_\n",
    "        totle_steps += 1\n",
    "        \n",
    "RL.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
